{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/dataset-cover.png\">\n",
    "\n",
    "# Contents\n",
    "* [Intro](#intro)\n",
    "* [The Project](#project)\n",
    "* [Data Prep](#prep)\n",
    "* [Building The Network](#network)\n",
    "* [What Is A Neural Network](#whatisanetwork)\n",
    "* [Building Blocks](#buildingblocks)\n",
    "* [Initialisation](#init)\n",
    "* [Forward Pass](#forward)\n",
    "* [Backward Pass](#backward)\n",
    "* [Update Weights](#update)\n",
    "* [Testing & Evaluation](#testing)\n",
    "* [Conclusion](#conclusion)\n",
    "* [References](#references)\n",
    "* [Other Resources](#resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Intro <a  id=\"intro\"></a>\n",
    "\n",
    "This notebook is written to illustraste how to build a simple 2 layer neural network from scratch! I have wanted to do this for a long time now as I notice that when I was learning about neural networks I had a tendency to skip straight to the cool stuff. Using great libraries like tensorflow and keras lets you build really cool projects with neural networks quickly and easily, which is great. However I noticed that there were big gaps in my understanding neural networks and I realised I didnt really understand their workings at a low level. This notebook was a great exercise in really understanding the low level concepts of neural networks and I hope that others will find it usefull as well. Any feedback or constructive criticism would be appreciated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Project <a  id=\"project\"></a>\n",
    "\n",
    "In this project we will be trying to diagnose whether a tumor found in breast cancer is malignent or benign. This will be using the Breast Cancer Wisconsin data set found <a href=\"https://www.kaggle.com/uciml/breast-cancer-wisconsin-data\">here</a>. \n",
    "\n",
    "This is a relatively small dataset with 569 entries. These datapoints contain 30 features that we will learn from and a binary classifcation of malignent \"M\" or benign \"B\". Like all ML models, this project is split up into 2 parts, data preparation and the building of the model. If you want to just skip to the code for the network head to [Building The Network](#network)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep <a  id=\"prep\"></a>\n",
    "\n",
    "All ML projects are different. Even if we use the same model, the data being used is rarely the same. Its also worth noting that good data is more important than a good model. Much like in nutrition, you cant out exercise a bad diet. If you're putting bad food into your body, your gonna see bad results. In ML it doesn't really matter how amazing and complicated your model is if you are feeding it bad data. Because of this, it is important to pay attention to our data preparation phase. This notebook just carrys out very basic data preparation but the Kaggle contributer Kaan Can has a very comprehensive tutorial that covers all the necessary skils for data science and has a whole section on cleaning data. The tutorial can be found <a href=\"https://www.kaggle.com/kanncaa1/data-sciencetutorial-for-beginners\">here</a>\n",
    "\n",
    "Now on to the actual code! The first thing we need to do is import any libraries such as pandas and then load in our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 33)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "      ...       texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0     ...               17.33           184.60      2019.0            0.1622   \n",
       "1     ...               23.41           158.80      1956.0            0.1238   \n",
       "2     ...               25.53           152.50      1709.0            0.1444   \n",
       "3     ...               26.50            98.87       567.7            0.2098   \n",
       "4     ...               16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "3                  0.17300          NaN  \n",
       "4                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "data = pd.read_csv('/floyd/input/breast-cancer-wisconsin/breastcancer.csv')\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the snapshot of our dataset we have 569 columns with 33 rows. After a quick glance through our data we can see that we dont need all of it. The last column is simply empty data so the first thing we will do is remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del data['Unnamed: 32']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to seperate our feature data and our targets. The features is the data we want to learn from in order and the targets is the diagnosis, wether or not the tumor is malignent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data.iloc[:, 2:].values\n",
    "y = data.iloc[:, 1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at our target labels, they are represented as either 'M' or 'B'. Now this makes the data easily readible for humans, but its actaully harder for a machine to interpret this. In order to make our labels more machine friendly we are going to change the labels to be either 1 or 0. Because im lazy, im going to use the sklearn libaray to quickly do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing we will look at is splitting up our data. Whenever we are training our models we need to divide our data into 2 sets, a training set and a testing set. To do this we randomly sample from our entire dataset and split the data with roughly a 70:30 ratio, but this can vary depending on the project. \n",
    "\n",
    "You might be thinking \"well, we dont have much data to begin with, why dont we just train our model with all of our data and get the most out of what we have?\" and thats a good question. The reason is that we dont want our model to overfit to the training data. \n",
    "\n",
    "Overfitting means that our model has gotten REALLY good at understanding and predicting the training data and can have a really high training accuracy. However, the model has ONLY learned about the training data and hasnt learned how to generalise. So when we introduce some new data, the model will perform poorly. Its like when you are studying for a test, but you have only studied the past exam papers. You have gotten really good at answering those specific questions, but when you are asked questions that didnt come up in the past exam papers, you cant answer them because you haven't really learned the topic. \n",
    "\n",
    "In order to identify if this is happening to our model we always train the model on a sub section of the data and then test the model on another portion of the dataset to ensure that it is learning correctly.\n",
    "\n",
    "Also, it is important that the data is randomly sampled, otherwise you might get data sets that contain highly correlated data and can effect your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final task we need to carry out is called feature scaling. At the minute our data is looking good and it makes a lot of sense to look at if you're a scientist or a doctor, but remember earlier we talked about how it is difficult for machines to understand human readable values? Although the data makes sense, we can convert our data so that all values are scaled to be within the same range. This makes it much faster for our network to train. \n",
    "\n",
    "We are going to use sklearns StandardScaler function to do this. This function will transform each variable of our data to have a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building The Network <a  id=\"network\"></a>\n",
    "\n",
    "<figure>\n",
    "    <img src=\"imgs/nn.png\" style=\"height:500px;\"/>\n",
    "    <figcaption style=\"text-align:center\">Fig.1 - Logical representation of what our deep neural network looks like </figcaption>\n",
    "</figure>\n",
    "\n",
    "So now that we have finally cleaned up our data we are ready dive into the good stuff! Neural Networks are an exciting area of machine learning and have grown massively over the past several years. However they can also be quite complicated and involve a lot of maths. This notebook will illustrate the building of the network and the theory behind, but wont go into too much detail on the low level maths. For more information on the math side of things checkout 3 blue 1 browns great <a href=\"https://www.youtube.com/watch?v=aircAruvnKk\">video</a> \n",
    "\n",
    "Below is the full project code. After the code is run and the results are displayed the notebook will explain each of the sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self,X, y, X_test, y_test, hidden_nodes=12, learning_rate=0.1, epochs=5000):\n",
    "        \n",
    "        #data\n",
    "        self.y = y[:,None]\n",
    "        self.X = X\n",
    "        \n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        \n",
    "        #parameters\n",
    "        np.random.seed(4)\n",
    "        self.input_nodes = len(X[0])\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = self.y.shape[1]\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        #init weights\n",
    "        self.w1 = 2*np.random.random((self.input_nodes, self.hidden_nodes)) - 1\n",
    "        self.w2 = 2*np.random.random((self.hidden_nodes, self.output_nodes)) - 1\n",
    "\n",
    "        self.train(epochs)\n",
    "        self.test()\n",
    "        \n",
    "    def sigmoid(self,X):\n",
    "        return (1/(1+np.exp(-X)))\n",
    "\n",
    "    def sigmoid_prime(self,X):\n",
    "        return X * (1 - X)\n",
    "        \n",
    "    def train(self, epochs):\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            \n",
    "            #FORWARD PROPAGATION\n",
    "            \n",
    "            #hidden layer\n",
    "            # W1(398,30) X(30,12)\n",
    "            l1 = self.sigmoid(np.dot(self.X, self.w1))\n",
    "\n",
    "            #output layer\n",
    "            #l1(398,12) W2(12,1)\n",
    "            l2 = self.sigmoid(np.dot(l1, self.w2))\n",
    "        \n",
    "            # BACKPROPAGATION\n",
    "            \n",
    "            #calculate how far off our prediciton was\n",
    "            error = self.y-l2\n",
    "            \n",
    "            #calculate how far off each layer is\n",
    "            l2_delta = error * self.sigmoid_prime(l2)\n",
    "            l1_delta = l2_delta.dot(self.w2.T) * self.sigmoid_prime(l1)\n",
    "\n",
    "            #update weights with our newly found error values\n",
    "            self.w2 = np.add(self.w2, l1.T.dot(l2_delta) * self.learning_rate)\n",
    "            self.w1 = np.add(self.w1, self.X.T.dot(l1_delta) * self.learning_rate)\n",
    "        \n",
    "        print('Error:', (abs(error)).mean())\n",
    "    \n",
    "    def test(self):\n",
    "        correct = 0\n",
    "        pred_list = []\n",
    "        \n",
    "        #replicate feedforward network for testing\n",
    "        l1 = self.sigmoid(np.dot(self.X_test, self.w1))\n",
    "        l2 = self.sigmoid(np.dot(l1, self.w2))\n",
    "        \n",
    "        #loop through all of the outputs of layer 2\n",
    "        for i in range(len(l2)):\n",
    "            if l2[i] >= 0.5:\n",
    "                pred = 1\n",
    "            else:\n",
    "                pred = 0\n",
    "\n",
    "            if pred == self.y_test[i]:\n",
    "                correct += 1\n",
    "                \n",
    "            pred_list.append(pred)\n",
    "\n",
    "        print(\"Test Accuracy: \", ((correct/len(y_test))*100),'%')\n",
    "        \n",
    "        #confusion matrix\n",
    "        cm = confusion_matrix(y_test, pred_list)\n",
    "        sns.heatmap(cm,annot=True)\n",
    "        plt.savefig('h.png')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.0038812214102433673\n",
      "Test Accuracy:  97.07602339181285 %\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAD8CAYAAABJsn7AAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npEIMYEkqxACWpEIMYEkqxACWpEL+D03JXTyurWgVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d069128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn = NeuralNetwork(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Is A Neural Network? <a  id=\"whatisanetwork\"></a>\n",
    "\n",
    "Like the name suggests, neural networks are designed after the architecture of our brains. Eletrical signals are sent through layers of neurons in our brains that are all connected together and eventually forms an output that our brain can understand and act on. Just like our brains architecture, our network has several layers made up of perceptron nodes(neurons).\n",
    "\n",
    "Each of these nodes simply holds a number. These nodes then have their numbers multiplied by the weights connecting them to the next layer. The data is inputed into the first layer of the network, there is a node for every feature in the data set. These inputs are passed through each layer of the network with the layers weights applied to them. After the inputs make it through the entire network we are left with our output. In this case we have a single output node that will be either 1 or 0(Malignent or Benign). This process is known as the forward pass. The next step is the backward pass. This is where the learning actually occurs. We find out how well our network predicted the outcome. We then move backward through the layers and updated the weights of the layers in order to improve our predictions. Nodes that are considered more important will end up having a higher weight value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Building Blocks <a  id=\"buildingblocks\"></a>\n",
    "\n",
    "Above is a very brief description of what is going in the neural network. Now we are going to look how to build it. The way I lik to think of implementing a neural network is to split it upon into several logical sections or building blocks. These are as follows:\n",
    "\n",
    "</br>\n",
    "<dl>\n",
    "<dd>1. Network Initialisation</dd>\n",
    "\n",
    "<dd>2. Forward Pass</dd>\n",
    "\n",
    "<dd>3. Backward Pass</dd>\n",
    "\n",
    "<dd>4. Update Weights</dd>\n",
    "\n",
    "<dd>5. Testing & Evaluation</dd>\n",
    "</dl>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Initialisation <a  id=\"init\"></a>\n",
    "\n",
    "The first thing we need to do is initialise our network. Here we will determine the structure of the network including the number of nodes for each layer(input, hidden and output), the learning rate, the number of epochs and the initial random weights for each of our layers. \n",
    "\n",
    "<dt>Input nodes</dt> \n",
    "this is the number of features for each entry in our data set, in this case it will be 30\n",
    "\n",
    "<dt>Hidden Nodes</dt> \n",
    "This is the number of nodes in our hidden layer. For this project I use 12 hidden nodes. This is an arbitrary number that I decided upon through experimenting with different values. \n",
    "\n",
    "<dt>Output Nodes</dt> \n",
    "This is simply how many outputs our network will have. In this case we are carrying out binary classification so our network can either be 0 or 1. Due to this we will only use 1 node. If we were building a classifier with more than 2 types (such as classifying different types of animals) we would have several output nodes.\n",
    "\n",
    "<dt>Learning Rate</dt> \n",
    "The learning rate determines how much we adjust our weights during the back pass. A high number will let our network make progress quickly, but as the learning progresses we might adjust our weights too much, and increase the error as opposed to decreasing it. A low learning rate will gradually learn and be able to generalise better, but will take longer to converge and requires more training epochs. A learning rate that isnt too high or too low is recomended to begin with and then you can adjust the learning rates based on the needs of the network. For this example I decided on a learning rate of 0.1 which is generally a good starting point.\n",
    "\n",
    "<dt>Epochs</dt>\n",
    "The number of epochs determines how many times we will train our network on the data set.\n",
    "\n",
    "<img src=\"imgs/init.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Forward Pass <a  id=\"forward\"></a>\n",
    "<figure>\n",
    "    <img src=\"imgs/forwardpass.png\" style=\"widght:300px;\">\n",
    "    <figcaption style=\"text-align:center;\">Example of a simple forward pass through a single layer</figcaption>\n",
    "</figure>\n",
    "\n",
    "<img src=\"imgs/forward.png\">\n",
    "\n",
    "As mentioned previously the forward pass is the prediction section of our network. We feed our input in through the layers of the network, applying the different weights as we go. This eventually leaves us with a prediciton. The steps in the forward pass are as follows:\n",
    "\n",
    "<dt>1) Get the input</dt> \n",
    "The input for the layer is the output of the previous layer, or in the input layer it is simply the training data.\n",
    "\n",
    "<dt>2) Apply the layer weights</dt> \n",
    "Now we apply the weights by getting the dot product of the inputs and the layers weights\n",
    "\n",
    "<dt>3) Activation function</dt> \n",
    "Once we apply the weights to the inputs we apply our activation function. In this project we are using the sigmoid function. This takes in a number and turns it into a value between 0 and 1. This function suits our needs are we are looking for a binary output\n",
    "<img src=\"imgs/sigmoid.png\">\n",
    "\n",
    "<img src=\"imgs/sigmoiddiagram.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Backward Pass <a  id=\"backward\"></a>\n",
    "\n",
    "<img src=\"imgs/back.png\">\n",
    "\n",
    "The backwards pass is the section where our network actually learns, this is known as backpropagation through gradient descent. Here we take our prediction and compare it with the actual true value. We then move backwards through our networks and update the weights of our layers in order to improve our predictions. There are several steps in the backwards pass of the network.\n",
    "\n",
    "<dt>1) Find the error of the network</dt> \n",
    "First we calculate how far off our prediction was. This is done by subtracting our preicted value from the true value\n",
    "\n",
    "<dt>2) Calculate the error for each of our layers</dt> \n",
    "Now that we know the error of the network we need to find the error for each of our layers. We do this by multiplying the error of the previous layer by the derivative of the current layer.\n",
    "\n",
    "The final layer (layer 2 in this case) will be using the network error as there is no previous layer. The derivative of the current layer is calculated using the sigmoid prime function shown below. The next layer uses the dot product of the previous error and the weights for the previous layer as the error. Once again this is multiplied by the sigmoid prime of the current layer\n",
    "\n",
    "<img src=\"imgs/sigmoid_prime.png\">\n",
    "<img src=\"imgs/sigmoid_prime_diagram.png\" style=\"height:300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Update Weights <a  id=\"update\"></a>\n",
    "\n",
    "Once we have completed our backward pass of the network we need to update our weights accordingly. For each layer of weights we need to carry out the following steps. \n",
    "\n",
    "<dt>1) Calculate how much to add to the layer weights</dt> \n",
    "for each layer of weights we need to calculate the adjustments to add to the weights. This is done by getting the dot product of the previous layer and the error of the current layer that we found in the back pass phase. We then multiply that dot product with our learning rate\n",
    "\n",
    "<dt>2) Add the adjustments to the current layer weights</dt> \n",
    "This is as simple as it sounds. Just add the adjustments we just calculated to the weights. \n",
    "\n",
    "For some of the calculations we need to use the transpose of the value (.T) this is just so the 2 matrices can be multiplied correctly. This can vary for each project.\n",
    "\n",
    "<img src=\"imgs/weights.png\">\n",
    "\n",
    "Congratulations you have just built the core model for your neural network! The only thing left to do is test the accuracy of our network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - Testing and Evaluation <a  id=\"testing\"></a>\n",
    "\n",
    "All we are doing here is recreating the forward pass in order to test how well our model works, except here we use our testing data instead of our training data. We also keep track of how many correct predictions we make and the list of predictions made.\n",
    "\n",
    "For each each prediction made we will compare it to the actual target and see if our model is correct, updating our statistic variables.\n",
    "\n",
    "We then print out the accuracy of our model to the terminal\n",
    "\n",
    "<img src=\"imgs/test.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion <a  id=\"conclusion\"></a>\n",
    "\n",
    "I hope this writeup is able to help others learn and understand neural networks. There is still a lot about the topic that wasn't covered here. This project simply looked at a very basic implemtation of a deep neural network. After writing this notebook I feel that I have gained a much better understanding of how neural networks actually work. This will be a big help when I move on to more complicated models using libraries such as tensorflow. \n",
    "\n",
    "Try playing around with this code and see what results you get. Better yet, try and recreate this project using a different data set. Try and figure out how to prepare the data, maybe add more hidden layers to improve the performance of the model. \n",
    "\n",
    "\n",
    "\n",
    "Below I have listed several great resources and references to the code and tutorials that I learned from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References <a  id=\"references\"></a>\n",
    "\n",
    "Preprocessing Data\n",
    "https://www.kaggle.com/thebrownviking20/intro-to-keras-with-breast-cancer-data-ann\n",
    "\n",
    "Basic Neural Network With Numpy\n",
    "https://www.kaggle.com/ancientaxe/simple-neural-network-from-scratch-in-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Resources <a  id=\"resources\"></a>\n",
    "\n",
    "A Neural Network in 11 lines of Python\n",
    "https://iamtrask.github.io/2015/07/12/basic-python-network/\n",
    "\n",
    "Siraj Raval Channel\n",
    "https://www.youtube.com/channel/UCWN3xxRkmTPmbKwht9FuE5A\n",
    "\n",
    "But what *is* a Neural Network? | Chapter 1, deep learning\n",
    "https://www.youtube.com/watch?v=aircAruvnKk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
